{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d88511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import nquad\n",
    "from keras.models import load_model\n",
    "import time\n",
    "\n",
    "\n",
    "# All punt plays of interest\n",
    "plays = pd.read_csv('../data/plays.csv').query('specialTeamsPlayType == \"Punt\"')\n",
    "punt_plays = plays[plays['specialTeamsResult']!='Non-Special Teams Result'][['gameId','playId','kickerId']]\n",
    "\n",
    "\n",
    "# Load all the models we will use to make predictions\n",
    "outcome_models = [] # Models for predicting probabilities for Fair Catch, Return, No Field outcomes\n",
    "field_models = [] # Models for predicting whether a punt is Fielded or not\n",
    "returnlen_models = [] # Models for predicting the return length of a punt\n",
    "for i in range(5):\n",
    "    outcome_models.append(load_model(f'./Models/ThreeOutcomeModel{i}.h5'))\n",
    "    field_models.append(load_model(f'./Models/FieldingModel{i}.h5'))\n",
    "    returnlen_models.append(load_model(f'./Models/ReturnModel{i}.h5'))\n",
    "    \n",
    "mdn_model = load_model('./Models/MDN_BivariateNorm.h5',compile=False) # Model for finding location of bouncing ball\n",
    "\n",
    "# Normalization statistics for input features for each model\n",
    "field_norm_stats = pd.read_csv('../ReducedData/training_statistics.csv').values # Outcome/Field models\n",
    "return_norm_stats = pd.read_csv('../ReducedData/return_training_statistics.csv').values # Return models\n",
    "bounce_norm_stats = pd.read_csv('../ReducedData/bounce_training_statistics.csv').values # Return models\n",
    "\n",
    "\n",
    "# Raw feature data for punts\n",
    "data = pd.read_csv('../ReducedData/ExtraPuntLocations.csv') # Untrained punts\n",
    "trained_data = pd.read_csv('../ReducedData/BC_AllPlayers.csv') # Raw data for the punt training data\n",
    "fb_land = pd.read_csv('../ReducedData/fb_land.csv') # Actual landing spot for all punts in the previous two files\n",
    "\n",
    "# The landing data is not corrected so all punts go to the right (larger X), so we must rotate the field\n",
    "fb_land_corrected_dir = fb_land.copy()\n",
    "fb_land_corrected_dir.loc[fb_land_corrected_dir['playDirection']=='left','x'] = 120-fb_land_corrected_dir.loc[fb_land_corrected_dir['playDirection']=='left','x']\n",
    "fb_land_corrected_dir.loc[fb_land_corrected_dir['playDirection']=='left','y'] = (160/3-fb_land_corrected_dir.loc[fb_land_corrected_dir['playDirection']=='left','y']).round(2)\n",
    "\n",
    "# Add landing location of the football and hangtime for the punt in untrained data\n",
    "data = data.merge(fb_land_corrected_dir[['gameId','playId','x','y','hangTime']]).rename({'x':'x_land','y':'y_land'},axis=1)\n",
    "\n",
    "# Combine all the raw feature information for the punts\n",
    "all_punt_data = pd.concat([trained_data,data],axis=0).drop(columns=['Type','specialTeamsResult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192a1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the magnitude of the velocity of a punt given its distance travelled and hangtime\n",
    "def calc_const(dx, dy, ht):\n",
    "    g = 32.1/3 # Gravitational constant in yards/s**2\n",
    "    dist = np.sqrt(dx**2 + dy**2)\n",
    "    v_const = (dist/ht)**2 + (g*ht/2)**2\n",
    "    return np.sqrt(v_const)\n",
    "\n",
    "# Calculate the hangtime of a punt with a given velocity (magnitude) and the distance travelled\n",
    "# Return two solutions corresponding to low-kick and high-kick solution\n",
    "def calc_time(del_x, v_const):\n",
    "    g = 32.1/3 # Gravitational constant in yards/s**2\n",
    "    p1 = 2/g**2*v_const**2\n",
    "    p2 = 2/g**2*np.sqrt(v_const**4 - g**2 * del_x**2)\n",
    "    return [np.sqrt(p1-p2),np.sqrt(p1+p2)]\n",
    "\n",
    "# Calculate the average velocity of an NFL punt from the punter's location and the landing spot\n",
    "dx = trained_data['x_punt'] - trained_data['x_land']\n",
    "dy = trained_data['y_punt'] - trained_data['y_land']\n",
    "ht = trained_data['hangTime']\n",
    "v_const = calc_const(dx,dy,ht)\n",
    "vc_avg = v_const.describe()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8643d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a year and punter to evaluate\n",
    "eval_year_punts = all_punt_data[all_punt_data['gameId']//1e6>=2020]\n",
    "num_punts_kicker = eval_year_punts['nflId_punt'].value_counts()\n",
    "\n",
    "min_punts = 16\n",
    "punts_to_eval = eval_year_punts[eval_year_punts['nflId_punt'].isin(num_punts_kicker[num_punts_kicker>=min_punts].index.values)]\n",
    "\n",
    "# Raw feature data for all punts of a particular punter\n",
    "kickerId_to_eval = 50975\n",
    "\n",
    "# Either evaluate all hypothetical punts by one particular punter, or evaluate the expected value of a real punt.\n",
    "#kicker_punts = punts_to_eval.query('nflId_punt == @kickerId_to_eval').sort_values(['gameId','playId'])\n",
    "kicker_punts = trained_data[trained_data['x_punt']>45].sample()\n",
    "\n",
    "# Consider a grid of possible landing positions for each punt\n",
    "x_loc = np.arange(11,110,1)\n",
    "y_loc = np.arange(1/6,160/3, 1)\n",
    "x_grid, y_grid = np.meshgrid(x_loc,y_loc)\n",
    "xy_mesh = pd.DataFrame(zip(x_grid.flatten(),y_grid.round(2).flatten()),columns=['x_land','y_land'])\n",
    "\n",
    "# Apply grid to each punt, keeping only punts that move 20-yards past the original line of scrimmage\n",
    "potential_land = kicker_punts.drop(columns=['x_land','y_land','hangTime']).merge(xy_mesh, how='cross')\n",
    "potential_land = potential_land[potential_land['x_land'] >= potential_land['x_fb'] + 20]\n",
    "potential_land['dist'] = np.sqrt((potential_land['x_land']-potential_land['x_punt'])**2 + (potential_land['y_land']-potential_land['y_punt'])**2)\n",
    "\n",
    "# Evaluate each punt-location based on what a league-average punter would achieve given the situation\n",
    "# Note, for hangtime calculation, we assume the larger hangtime solution\n",
    "potential_land = potential_land[potential_land['dist'] <= 3*vc_avg**2/32.1]\n",
    "_,potential_land['hangTime'] = calc_time(potential_land['dist'],vc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e138e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several Functions that will be useful in converting the raw features of a punt to the specific features of each model\n",
    "\n",
    "\n",
    "# Calculate the array of coordinate differences between two sets of teams\n",
    "def diff_coord(team1, team2):\n",
    "    t1 = team1.stack().reset_index()\n",
    "    t2 = team2.stack().reset_index()\n",
    "    t1xt2 = t1.merge(t2,on='level_0')\n",
    "    t1xt2['diff'] = t1xt2['0_y'] - t1xt2['0_x']\n",
    "    return t1xt2.set_index(['level_0','level_1_y','level_1_x'])['diff']\n",
    "\n",
    "\n",
    "# Calculate the array of coordinate differences between a particular team and some coordinate\n",
    "def diff_team_from_loc(team, coord):\n",
    "    return -team.sub(coord,axis=0)\n",
    "\n",
    "\n",
    "# Standardize features given the normalization statistics\n",
    "def standardize_features(tensor, norm_stats):\n",
    "    norm_tensor = np.copy(tensor)\n",
    "    for i in range(tensor.shape[-1]):\n",
    "        norm_tensor[...,i] = (norm_tensor[...,i] - norm_stats[i,0])/norm_stats[i,1]\n",
    "    return norm_tensor\n",
    "\n",
    "\n",
    "# Isolate each teams x and y coordinates\n",
    "def team_coords(df):\n",
    "    punt_x = df.iloc[:,5:24:2]\n",
    "    punt_y = df.iloc[:,6:25:2]\n",
    "    ret_x = df.iloc[:,25:46:2]\n",
    "    ret_y = df.iloc[:,26:47:2]\n",
    "    return [punt_x,punt_y,ret_x,ret_y]\n",
    "\n",
    "\n",
    "# Convert the raw data into the specific features used in the outcome_models and field_models\n",
    "# Predict probability of a punt being fielded, fair caught, returned, or not fielded\n",
    "def convert_to_field_ft(df):\n",
    "    features = np.zeros((len(df),11,10,9))\n",
    "    punt_x,punt_y,ret_x,ret_y = team_coords(df)\n",
    "    \n",
    "    features[:,:,:,0] = diff_coord(punt_x,ret_x).unstack().values.reshape(len(df),11,10)\n",
    "    features[:,:,:,1] = diff_coord(punt_y,ret_y).unstack().values.reshape(len(df),11,10)\n",
    "    \n",
    "    features[:,:,:,2] = np.tile((diff_team_from_loc(ret_x, df['x_punt']).values)[:,:,np.newaxis],10)\n",
    "    features[:,:,:,3] = np.tile((diff_team_from_loc(ret_y, df['y_punt']).values)[:,:,np.newaxis],10)\n",
    "    \n",
    "    features[:,:,:,4] = np.tile((diff_team_from_loc(ret_x, df['x_land']).values)[:,:,np.newaxis],10)\n",
    "    features[:,:,:,5] = np.tile((diff_team_from_loc(ret_y, df['y_land']).values)[:,:,np.newaxis],10)\n",
    "    \n",
    "    features[:,:,:,6] = np.tile(df['x_fb'].values,(10,11,1)).T\n",
    "    features[:,:,:,7] = np.tile(df['y_fb'].values,(10,11,1)).T\n",
    "    \n",
    "    features[:,:,:,8] = np.tile(df['hangTime'].values,(10,11,1)).T\n",
    "    \n",
    "    return standardize_features(features, field_norm_stats)\n",
    "\n",
    "\n",
    "# Convert the raw data into the specific features used in the returnlen_models\n",
    "# Length of return predictions\n",
    "def convert_to_return_ft(df):\n",
    "    features = np.zeros((len(df),10,11,7))\n",
    "    punt_x,punt_y,ret_x,ret_y = team_coords(df)\n",
    "    \n",
    "    features[:,:,:,0] = diff_coord(ret_x,punt_x).unstack().values.reshape(len(df),10,11)\n",
    "    features[:,:,:,1] = diff_coord(ret_y,punt_y).unstack().values.reshape(len(df),10,11)\n",
    "    \n",
    "    features[:,:,:,2] = np.tile((diff_team_from_loc(ret_x, df['x_punt']).values)[:,np.newaxis],(1,10,1))\n",
    "    features[:,:,:,3] = np.tile((diff_team_from_loc(ret_y, df['y_punt']).values)[:,np.newaxis],(1,10,1))\n",
    "    \n",
    "    features[:,:,:,4] = np.tile((diff_team_from_loc(punt_x, df['x_land']).values)[:,:,np.newaxis],11)\n",
    "    features[:,:,:,5] = np.tile((diff_team_from_loc(punt_y, df['y_land']).values)[:,:,np.newaxis],11)\n",
    "    \n",
    "    features[:,:,:,6] = np.tile(df['hangTime'].values,(11,10,1)).T\n",
    "    \n",
    "    return standardize_features(features, return_norm_stats)\n",
    "\n",
    "\n",
    "# Convert the raw data into the specific features used in the mdn_model\n",
    "# Bounce location statistic predictions\n",
    "def convert_to_bounce_ft(df):\n",
    "    features = np.zeros((len(df),5))\n",
    "    \n",
    "    dx = (df['x_land']-df['x_punt']).values\n",
    "    dy = (df['y_land']-df['y_punt']).values\n",
    "    v_c = calc_const(dx, dy, df['hangTime'].values)\n",
    "    dist = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    features[:,0] = df['x_fb'].values\n",
    "    features[:,1] = (df['y_land'] - df['y_punt']).values\n",
    "    features[:,2] = df['hangTime'].values\n",
    "    features[:,3] = v_c\n",
    "    features[:,4] = dist/(v_c*df['hangTime'].values)\n",
    "    \n",
    "    return standardize_features(features, bounce_norm_stats)\n",
    "\n",
    "\n",
    "# Return the three multi-dim arrays to be fed into models for predictions\n",
    "# outcome_models/field_models , returnlen_models , mdn_model\n",
    "def get_model_features(df):\n",
    "    return [convert_to_field_ft(df),convert_to_return_ft(df),convert_to_bounce_ft(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2668ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert punts into features and get model predictions for each\n",
    "field_ft, return_ft, bounce_ft = get_model_features(potential_land)\n",
    "\n",
    "num_models = 5\n",
    "field_pred = np.zeros((field_ft.shape[0], 4*num_models))\n",
    "return_pred = np.zeros((return_ft.shape[0], num_models))\n",
    "bounce_pred = mdn_model.predict(bounce_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496c10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_models):\n",
    "    # Each model in outcome_models will write to the array in the form (P_FairCatch, P_NoField, P_Return)\n",
    "    # Each model in field_models will write to the array  1-P_Field\n",
    "    # The field prediction array will have each pair of models recording (P_FC, P_NF, P_R, 1-P_F)\n",
    "    field_pred[:,4*i:4*i+3] = outcome_models[i].predict(field_ft)\n",
    "    field_pred[:,4*i+3] = field_models[i].predict(field_ft).reshape(-1,)\n",
    "    \n",
    "    # Return length will just record each model's prediction\n",
    "    return_pred[:,i] = returnlen_models[i].predict(return_ft).reshape(-1,)\n",
    "\n",
    "    \n",
    "\n",
    "# For the fielding probabilities, we take an ensemble average over the model predictions\n",
    "ens_avg_pred = np.zeros((field_pred.shape[0],4))\n",
    "\n",
    "# Average probability that the punt will not be fielded\n",
    "ens_avg_pred[:,0] = 1/10*(field_pred[:,1::4].sum(axis=1) + field_pred[:,3::4].sum(axis=1))\n",
    "\n",
    "# Average probability that the punt will be fielded\n",
    "# i.e. [0] + [1] = 1.00\n",
    "ens_avg_pred[:,1] = 1/10*((1-field_pred[:,3::4]).sum(axis=1) + field_pred[:,0::4].sum(axis=1) + field_pred[:,2::4].sum(axis=1))\n",
    "\n",
    "# Average probability for a Fair Catch (2) and a return (3)\n",
    "# i.e. [2] + [3] = [1]\n",
    "ens_avg_pred[:,2] = ens_avg_pred[:,1] * field_pred[:,0::4].sum(axis=1)/(field_pred[:,0::4].sum(axis=1)+field_pred[:,2::4].sum(axis=1))\n",
    "ens_avg_pred[:,3] = ens_avg_pred[:,1] * field_pred[:,2::4].sum(axis=1)/(field_pred[:,0::4].sum(axis=1)+field_pred[:,2::4].sum(axis=1))\n",
    "\n",
    "# Combine fielding and return predictions for each punt into a DataFrame\n",
    "ens_df = pd.concat([potential_land[['gameId','playId','x_fb','y_fb','x_land','y_land']].reset_index(drop=True),pd.DataFrame(ens_avg_pred, columns=['NF_avg','Field_avg','FC_avg','R_avg'])],axis=1)\n",
    "ens_df['R_len'] = np.sinh(return_pred).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16a172ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change of basis from x-y coordinates into those defined by the direction of the punt (punter -> landing location)\n",
    "cos = ((potential_land['x_land'] - potential_land['x_punt'])/potential_land['dist']).values\n",
    "sin = ((potential_land['y_land'] - potential_land['y_punt'])/potential_land['dist']).values\n",
    "\n",
    "# Mean predicted bounce length in the punt-direction coordinates\n",
    "mu_v = bounce_pred[:,0]\n",
    "mu_vp = bounce_pred[:,1]\n",
    "\n",
    "# Roughly approximate the expected x,y values of the bounces\n",
    "ens_df['x_exp_bounce'] = mu_v*cos - mu_vp*sin\n",
    "ens_df['y_exp_bounce'] = mu_v*sin + mu_v*cos\n",
    "\n",
    "# Additional statistics about the punt and bounce\n",
    "ens_df['x_to_ez'] = 110 - ens_df['x_land']\n",
    "ens_df['y_to_oob'] = (80/3-np.abs(80/3 - ens_df['y_land'])) * np.sign(ens_df['y_land']-80/3)\n",
    "ens_df['y_rat'] = ens_df['y_to_oob']/ens_df['y_exp_bounce']\n",
    "ens_df['x_exp_notb'] = ens_df['x_exp_bounce']*ens_df['y_rat']\n",
    "ens_df.loc[(ens_df['y_rat']>1)|(ens_df['y_rat']<0),'x_exp_notb'] = ens_df['x_exp_bounce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24f5f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounce model outputs statistics of the bivariate normal distributions\n",
    "# Functions to be used in calculating the probabilities and expected values\n",
    "\n",
    "# Standardize the random variables in the exponential of distributions\n",
    "def standardize(x,mu,sigma):\n",
    "    return (x-mu)/sigma\n",
    "\n",
    "# Given a set of distribution parameters, return the bivariate normal distribution function\n",
    "def get_bivnorm(mx,my, sx,sy, r):\n",
    "    def biv_norm(x,y):\n",
    "        x_stand = standardize(x,mx,sx)\n",
    "        y_stand = standardize(y,my,sy)\n",
    "        return 1/(2*np.pi*sx*sy*np.sqrt(1-r**2)) * np.exp(-1/(2*(1-r**2))*(x_stand**2+y_stand**2+2*r*x_stand*y_stand))\n",
    "    return biv_norm\n",
    "\n",
    "# Rotate on-field coordinates to punt-direction coordinates before acquiring PDF\n",
    "def get_rot_pdf(mx,my, sx,sy, r, ctheta,stheta):\n",
    "    pdf = get_bivnorm(mx,my, sx,sy, r)\n",
    "    def rotated_biv_norm(x,y):\n",
    "        x_rot = x*ctheta + y*stheta\n",
    "        y_rot = y*ctheta - x*stheta\n",
    "        return pdf(x_rot,y_rot)\n",
    "    return rotated_biv_norm\n",
    "\n",
    "# Calculate the expected value of x-field coordinate\n",
    "def get_rot_ExpX(mx,my, sx,sy, r, ctheta,stheta):\n",
    "    pdf = get_bivnorm(mx,my, sx,sy, r)\n",
    "    def rotated_expX(x,y):\n",
    "        x_rot = x*ctheta + y*stheta\n",
    "        y_rot = y*ctheta - x*stheta\n",
    "        return x * pdf(x_rot,y_rot)\n",
    "    return rotated_expX\n",
    "\n",
    "# Calculate expected value of x-field coordinate when the ball bounces out of bound\n",
    "# Assuming a straight path from the bounce location to the final location, the x-value is truncated when the ball leaves play\n",
    "def get_rot_truncatedExpX(mx,my, sx,sy, r, ctheta,stheta, y_thres):\n",
    "    pdf = get_bivnorm(mx,my, sx,sy, r)\n",
    "    def rotated_truncexpX(x,y):\n",
    "        x_rot = x*ctheta + y*stheta\n",
    "        y_rot = y*ctheta - x*stheta\n",
    "        return x*y_thres/y * pdf(x_rot,y_rot)\n",
    "    return rotated_truncexpX\n",
    "\n",
    "# Returns y-field integration bounds depending on whether the punt is closer to the y = 0 or y = 160/3 sidelines\n",
    "def y_oob(y_togo):\n",
    "    ranges = [[-np.inf,y_togo],[y_togo,np.inf]]\n",
    "    if y_togo>0:\n",
    "        return ranges\n",
    "    return [ranges[1],ranges[0]]\n",
    "\n",
    "# Return x-field integration bounds, for integrating both touchbacks and out of bounds\n",
    "def get_XBounds(x_ez,y_oob):\n",
    "    slope = x_ez/y_oob\n",
    "    def tb_xbound(y):\n",
    "        x_minbound = slope*(y-y_oob)+x_ez\n",
    "        return [x_minbound, np.inf]\n",
    "\n",
    "    def oob_xbound(y):\n",
    "        x_upbound = slope*(y-y_oob)+x_ez\n",
    "        return [x_ez, x_upbound]\n",
    "    return [tb_xbound,oob_xbound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64d59421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 21) (4, 21)\n"
     ]
    }
   ],
   "source": [
    "ens_df['p_tb'] = 0\n",
    "ens_df['ex_ntb'] = (1-ens_df['p_tb'])*ens_df['x_exp_notb']\n",
    "\n",
    "# The Field Value is the absolute yardline of the average position of the ball after the punt\n",
    "# Fair Catch : Value -> x_land\n",
    "# Return : Value -> x_land - return_length\n",
    "# No Field : Touchback Value -> 90 = x_land + (90 - x_land)\n",
    "# No Field : Other, Value -> x_land + x_bounce\n",
    "ens_df['Field_Val'] = ens_df['x_land'] - ens_df['R_len']*ens_df['R_avg'] + ens_df['NF_avg']*(ens_df['p_tb']*(90-ens_df['x_land']) + ens_df['ex_ntb'])\n",
    "\n",
    "ens_df['has_calcd'] = False\n",
    "\n",
    "\n",
    "# Split the punts into two categories, based on how quickly the algorithm can calculate the true value\n",
    "fb_split = 55\n",
    "max_FV = ens_df.sort_values('x_fb',ascending=False).drop_duplicates(subset=['gameId','playId']).sort_values(['gameId','playId'])\n",
    "midfield_punts = ens_df.reset_index().merge(max_FV.query('x_fb < @fb_split')[['gameId','playId']]).set_index('index')\n",
    "pin_punts = ens_df.reset_index().merge(max_FV.query('x_fb >= @fb_split')[['gameId','playId']]).set_index('index')\n",
    "\n",
    "print(midfield_punts.drop_duplicates(subset=['gameId','playId']).shape, pin_punts.drop_duplicates(subset=['gameId','playId']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0151c957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2020110810 775\n",
      "Field Value (62): 89.80 -> 89.01\n",
      "Field Value (259): 89.18 -> 89.03\n",
      "93.00, 7.17, 89.03\n",
      "Eval Time: 11.5\n",
      "\n",
      "1 2020110810 3645\n",
      "Field Value (3047): 104.07 -> 97.72\n",
      "Field Value (3285): 103.41 -> 98.26\n",
      "Field Value (3417): 102.31 -> 98.80\n",
      "Field Value (3449): 100.63 -> 98.85\n",
      "Field Value (3481): 100.40 -> 99.00\n",
      "Field Value (3513): 100.13 -> 99.14\n",
      "Field Value (3545): 99.78 -> 99.17\n",
      "101.00, 51.17, 99.17\n",
      "Eval Time: 162.7\n",
      "\n",
      "2 2020112601 3413\n",
      "Field Value (4764): 93.79 -> 93.19\n",
      "Field Value (5110): 93.64 -> 93.20\n",
      "97.00, 42.17, 93.20\n",
      "Eval Time: 17.1\n",
      "\n",
      "3 2020120800 293\n",
      "Field Value (5951): 98.33 -> 96.03\n",
      "Field Value (5780): 97.79 -> 96.11\n",
      "Field Value (5647): 97.45 -> 96.21\n",
      "99.00, 5.17, 96.21\n",
      "Eval Time: 61.4\n",
      "\n",
      "4 2020121302 2370\n",
      "Field Value (8381): 104.43 -> 97.76\n",
      "Field Value (7786): 104.41 -> 97.83\n",
      "Field Value (7581): 103.64 -> 98.41\n",
      "Field Value (7448): 102.82 -> 99.12\n",
      "Field Value (7351): 102.01 -> 99.49\n",
      "101.00, 2.17, 99.49\n",
      "Eval Time: 166.5\n",
      "\n",
      "5 2020122013 268\n",
      "Field Value (10782): 94.89 -> 93.93\n",
      "Field Value (10979): 94.68 -> 93.93\n",
      "98.00, 7.17, 93.93\n",
      "Eval Time: 23.7\n",
      "\n",
      "6 2020122013 2466\n",
      "Field Value (14350): 90.42 -> 89.92\n",
      "Field Value (15004): 90.36 -> 90.07\n",
      "Field Value (15697): 90.35 -> 90.11\n",
      "95.00, 43.17, 90.11\n",
      "Eval Time: 15.2\n",
      "\n",
      "7 2020122013 3992\n",
      "Field Value (16081): 87.38 -> 86.76\n",
      "Field Value (16178): 87.02 -> 86.83\n",
      "Field Value (17615): 87.02 -> 87.05\n",
      "90.00, 46.17, 87.05\n",
      "Eval Time: 5.8\n",
      "\n",
      "8 2020122712 733\n",
      "Field Value (17888): 94.42 -> 93.29\n",
      "Field Value (18117): 94.30 -> 93.63\n",
      "97.00, 8.17, 93.63\n",
      "Eval Time: 14.3\n",
      "\n",
      "9 2020122712 3080\n",
      "Field Value (19657): 82.90 -> 81.74\n",
      "Field Value (19688): 82.49 -> 81.81\n",
      "Field Value (19751): 82.19 -> 81.90\n",
      "Field Value (21174): 82.17 -> 82.12\n",
      "85.00, 45.17, 82.12\n",
      "Eval Time: 20.9\n",
      "\n",
      "10 2020122712 4081\n",
      "Field Value (22738): 78.79 -> 77.00\n",
      "Field Value (22797): 78.55 -> 77.80\n",
      "79.00, 2.17, 77.80\n",
      "Eval Time: 20.1\n",
      "\n",
      "11 2021010308 414\n",
      "Field Value (25926): 100.69 -> 97.17\n",
      "Field Value (26096): 100.10 -> 97.39\n",
      "Field Value (26195): 98.34 -> 97.48\n",
      "100.00, 49.17, 97.48\n",
      "Eval Time: 65.3\n",
      "\n",
      "12 2021010308 952\n",
      "Field Value (26351): 84.50 -> 82.86\n",
      "Field Value (27906): 84.46 -> 84.48\n",
      "89.00, 46.17, 84.48\n",
      "Eval Time: 5.2\n",
      "\n",
      "13 2021010308 1174\n",
      "Field Value (29720): 83.55 -> 83.51\n",
      "Field Value (29374): 83.55 -> 83.53\n",
      "90.00, 35.17, 83.53\n",
      "Eval Time: 4.6\n",
      "\n",
      "14 2021010308 1357\n",
      "Field Value (30009): 71.23 -> 69.69\n",
      "Field Value (30039): 70.96 -> 69.99\n",
      "Field Value (30070): 70.67 -> 70.06\n",
      "Field Value (31549): 70.52 -> 70.67\n",
      "73.00, 46.17, 70.67\n",
      "Eval Time: 11.6\n",
      "\n",
      "15 2021010308 2461\n",
      "Field Value (33316): 86.33 -> 86.27\n",
      "89.00, 45.17, 86.27\n",
      "Eval Time: 1.0\n",
      "\n",
      "Total Eval Time: 606.8\n"
     ]
    }
   ],
   "source": [
    "# For each punt, loop over highest field value locations and perform the full integration of expected values\n",
    "# If the Field Value remains maximal, the location is the true maximum field value\n",
    "# This works since we over-approximating the Field Value with earlier approximation\n",
    "# When the full evaluation remains maximal, or we arrive at a point which we have already calculated, we are done\n",
    "\n",
    "games_to_eval = midfield_punts.copy()\n",
    "optimal_values = np.zeros((games_to_eval.drop_duplicates(subset=['gameId','playId']).shape[0],6))\n",
    "# index, x_land, y_land, Field Value, P_TB, ex_ntb\n",
    "\n",
    "start_all = time.perf_counter()\n",
    "for i,((g,p), land_stats) in enumerate(games_to_eval.groupby(['gameId','playId'])):\n",
    "    print(i,g,p)\n",
    "    highest_newVal = 0\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    converge = False\n",
    "    while not converge:\n",
    "        best_id = land_stats.sort_values('Field_Val',ascending=False).head(1).index[0]\n",
    "        \n",
    "        xl = land_stats.loc[best_id,'x_land']\n",
    "        yl = land_stats.loc[best_id,'y_land']\n",
    "    \n",
    "        x_to_ez = land_stats.loc[best_id,'x_to_ez']\n",
    "        y_to_oob = land_stats.loc[best_id, 'y_to_oob']\n",
    "        fv_old = land_stats.loc[best_id,'Field_Val']\n",
    "        \n",
    "        # Only evaluate the integral if we have not explicitly integrated this point already\n",
    "        if not land_stats.loc[best_id, 'has_calcd']:\n",
    "            y_bounds = y_oob(y_to_oob)\n",
    "            x_bound_tb, x_bound_oob = get_XBounds(x_to_ez, y_to_oob)\n",
    "\n",
    "            # Get relevant functionals\n",
    "            rotated_PDF = get_rot_pdf(*bounce_pred[best_id], cos[best_id], sin[best_id])\n",
    "            rotated_ExpX = get_rot_ExpX(*bounce_pred[best_id], cos[best_id], sin[best_id])\n",
    "            rot_trunc_ExpX = get_rot_truncatedExpX(*bounce_pred[best_id], cos[best_id], sin[best_id],y_to_oob)\n",
    "            \n",
    "            # Evaluate the integrals\n",
    "            # Note, it is faster to split the conditional integration bounds due to the decrease in function calls\n",
    "            ex_ip = nquad(rotated_ExpX, [[-110,x_to_ez],y_bounds[0]])[0]\n",
    "            p_tb_new = nquad(rotated_PDF, [[x_to_ez,np.inf],y_bounds[0]])[0] + nquad(rotated_PDF, [x_bound_tb,y_bounds[1]])[0]\n",
    "            ex_oob = nquad(rot_trunc_ExpX,[[-110,x_to_ez],y_bounds[1]])[0] + nquad(rot_trunc_ExpX,[x_bound_oob,y_bounds[1]])[0]\n",
    "\n",
    "            # When evaluating a point near the sideline, can update touchback probabilities in the middle of the field\n",
    "            # Unless already explicitly evaluated those locations\n",
    "            # The logic is identical to the previous step\n",
    "            # Note, this is only a useful speed-up when punting near the endzone\n",
    "            interior_locations = land_stats.loc[\n",
    "                (land_stats['x_land']==xl) \\\n",
    "                & (~land_stats['has_calcd']) \\\n",
    "                & (land_stats['y_land'].between(*sorted([80/3, land_stats.loc[best_id, 'y_land']]),inclusive='both'))].index.values\n",
    "\n",
    "            # Update the integrated values for locations closer to the middle of the field than the current location\n",
    "            land_stats.loc[interior_locations, 'p_tb'] = p_tb_new\n",
    "\n",
    "            # E(X|no TB) * P(no TB) unsure whether it should generally increase as we move towards from the middle of the field\n",
    "            # Therefore, do not update this value except when exactly calculating it\n",
    "            land_stats.loc[best_id, 'ex_ntb'] = (ex_ip + ex_oob)\n",
    "            \n",
    "            # Update expected value of field position for any interior point\n",
    "            land_stats.loc[interior_locations,'Field_Val'] = land_stats.loc[interior_locations,'x_land'] \\\n",
    "                    - land_stats.loc[interior_locations,'R_avg']*land_stats.loc[interior_locations,'R_len'] \\\n",
    "                    + land_stats.loc[interior_locations,'NF_avg']*( \\\n",
    "                                land_stats.loc[interior_locations,'p_tb']*(90-land_stats.loc[interior_locations,'x_land']) \\\n",
    "                                + land_stats.loc[interior_locations,'ex_ntb'])\n",
    "\n",
    "        land_stats.loc[best_id, 'has_calcd'] = True\n",
    "        fv = land_stats.loc[best_id,'Field_Val']\n",
    "        \n",
    "        # Print message when we have a local maxima in checked values\n",
    "        # As Old - New goes to zeros, the current loop is closer to finishing\n",
    "        if fv > highest_newVal:\n",
    "            highest_newVal = fv\n",
    "            print(f'Field Value ({best_id}): {fv_old:.2f} -> {fv:.2f}')\n",
    "\n",
    "        if best_id == land_stats.sort_values('Field_Val',ascending=False).head(1).index[0]:\n",
    "            end = time.perf_counter()\n",
    "            print(f'{xl:.2f}, {yl:.2f}, {fv:.2f}')\n",
    "            optimal_values[i, 0] = best_id\n",
    "            optimal_values[i, 1] = xl\n",
    "            optimal_values[i, 2] = yl\n",
    "            optimal_values[i, 3] = fv\n",
    "            optimal_values[i, 4] = land_stats.loc[best_id,'p_tb']\n",
    "            optimal_values[i, 5] = land_stats.loc[best_id,'ex_ntb']\n",
    "            converge = True\n",
    "\n",
    "    print(f'Eval Time: {end - start:.1f}')\n",
    "    print()\n",
    "\n",
    "end_all = time.perf_counter()\n",
    "print(f'Total Eval Time: {end_all - start_all:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b66c5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 Time: 0.00\n",
      "0.62 Time: 90.96\n",
      "Total Eval Time: 136.0\n"
     ]
    }
   ],
   "source": [
    "sideline_locs = pin_punts.reset_index().query('x_land >= 100').sort_values('y_land').groupby(['gameId','playId','x_land']).agg(['first', 'last']).stack().reset_index().drop(columns='level_3').set_index('index').copy()\n",
    "\n",
    "start_all = time.perf_counter()\n",
    "start = time.perf_counter()\n",
    "for i, cur_id in enumerate(sideline_locs.index.values):\n",
    "    if i%50 == 0:\n",
    "        end = time.perf_counter()\n",
    "        print(f'{(i/len(sideline_locs)):.2f} Time: {end-start:.2f}')\n",
    "        start = time.perf_counter()\n",
    "\n",
    "    x_to_ez = sideline_locs.loc[cur_id,'x_to_ez']\n",
    "    y_to_oob = sideline_locs.loc[cur_id, 'y_to_oob']\n",
    "\n",
    "    y_bounds = y_oob(y_to_oob)\n",
    "    x_bound_tb, x_bound_oob = get_XBounds(x_to_ez, y_to_oob)\n",
    "\n",
    "    # Get relevant functionals\n",
    "    rotated_PDF = get_rot_pdf(*bounce_pred[cur_id], cos[cur_id], sin[cur_id])\n",
    "    rotated_ExpX = get_rot_ExpX(*bounce_pred[cur_id], cos[cur_id], sin[cur_id])\n",
    "    rot_trunc_ExpX = get_rot_truncatedExpX(*bounce_pred[cur_id], cos[cur_id], sin[cur_id],y_to_oob)\n",
    "\n",
    "    # Evaluate the integrals\n",
    "    # Note, it is faster to split the conditional integration bounds due to the decrease in function calls\n",
    "    ex_ip = nquad(rotated_ExpX, [[-110,x_to_ez],y_bounds[0]])[0]\n",
    "    p_tb_new = nquad(rotated_PDF, [[x_to_ez,np.inf],y_bounds[0]])[0] + nquad(rotated_PDF, [x_bound_tb,y_bounds[1]])[0]\n",
    "    ex_oob = nquad(rot_trunc_ExpX,[[-110,x_to_ez],y_bounds[1]])[0] + nquad(rot_trunc_ExpX,[x_bound_oob,y_bounds[1]])[0]\n",
    "\n",
    "    # Update the integrated values for locations closer to the middle of the field than the current location\n",
    "    sideline_locs.loc[cur_id, 'p_tb'] = p_tb_new\n",
    "\n",
    "    # E(X|no TB) * P(no TB) unsure whether it should generally increase as we move towards from the middle of the field\n",
    "    # Therefore, do not update this value except when exactly calculating it\n",
    "    sideline_locs.loc[cur_id, 'ex_ntb'] = (ex_ip + ex_oob)\n",
    "    \n",
    "    \n",
    "    sideline_locs.loc[cur_id, 'has_calcd'] = True\n",
    "    \n",
    "sideline_locs['Field_Val'] = sideline_locs['x_land'] - sideline_locs['R_avg']*sideline_locs['R_len'] + sideline_locs['NF_avg']*(sideline_locs['p_tb']*(90-sideline_locs['x_land']) + sideline_locs['ex_ntb'])\n",
    "\n",
    "end_all = time.perf_counter()\n",
    "print(f'Total Eval Time: {end_all - start_all:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7986bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "midfield_locs = midfield_punts.copy()\n",
    "midfield_locs.loc[optimal_values[:,0],['Field_Val','p_tb','ex_ntb']] = optimal_values[:,3:]\n",
    "midfield_locs.loc[optimal_values[:,0], 'has_calcd'] = True\n",
    "check_low = midfield_locs.loc[optimal_values[:,0]]\n",
    "\n",
    "check_high = sideline_locs.sort_values('Field_Val',ascending=False).drop_duplicates(subset=['gameId','playId']).sort_values(['gameId','playId'])\n",
    "\n",
    "pd.concat([check_low,check_high]).to_csv(f'../PredictedData/OptimalLocation/OptLoc_{kickerId_to_eval}.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aed349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFLBigData",
   "language": "python",
   "name": "nflbigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
